{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Methodology**\n",
        "\n",
        "Our analysis pipeline was implemented in Python, leveraging several specialized libraries:\n",
        "\n",
        "*   spaCy: For German-language natural language processing and dependency parsing\n",
        "*   pandas: For data manipulation and management\n",
        "*   matplotlib & seaborn: For visualization generation\n",
        "*   Regular expressions (re): For pattern matching\n",
        "*   Collections.Counter: For efficient frequency counting\n",
        "\n",
        "The system was designed to be robust to common challenges in German text analysis, namely handling compound nouns and grammatical gender, accounting for different verb forms and tenses, managing sentence boundary detection in complex German sentences, and recognizing entities despite variations in naming conventions\n",
        "\n",
        "# Volume, Keyword and Contextual Analysis\n",
        "\n",
        "The objective of our volume, keyword, and contextual analysis is to determine how, and to what extent, different news outlets have reported on specific topics, as well as to identify the words that commonly appear in connection with these topics. This analysis serves as a foundation for understanding the contextual usage of keywords throughout our study.\n",
        "\n",
        "At this stage, the analysis does not yet allow for a precise interpretation of the semantic relationships between keywords and their context. However, it does reveal the underlying linguistic narratives through which specific topics are presented.\n",
        "\n",
        "Methodology\n",
        "Implementation Details\n",
        "Our analysis pipeline was implemented in Python, leveraging several specialized libraries:\n",
        "spaCy: For German-language natural language processing and dependency parsing\n",
        "pandas: For data manipulation and management\n",
        "matplotlib & seaborn: For visualization generation\n",
        "Regular expressions (re): For pattern matching\n",
        "Collections.Counter: For efficient frequency counting\n",
        "The system was designed to be robust to common challenges in German text analysis, namely handling compound nouns and grammatical gender, accounting for different verb forms and tenses, managing sentence boundary detection in complex German sentences, and recognizing entities despite variations in naming conventions\n",
        "Volume, Keyword and Contextual Analysis\n",
        "\n",
        "The objective of our volume, keyword, and contextual analysis is to determine how, and to what extent, different news outlets have reported on specific topics, as well as to identify the words that commonly appear in connection with these topics. This analysis serves as a foundation for understanding the contextual usage of keywords throughout our study.\n",
        "At this stage, the analysis does not yet allow for a precise interpretation of the semantic relationships between keywords and their context. However, it does reveal the underlying linguistic narratives through which specific topics are presented.\n",
        "\n",
        "Article Volume and Trend by Source\n",
        "\n",
        "In a first step, the reporting activity of individual news outlets is visualized over time using a line chart broken down by month. This allows for the analysis of both the volume and trend of coverage across the given time period.\n",
        "\n",
        "artikel_pro_monat = df.resample('M').size()\n",
        "artikel_pro_monat.plot()\n",
        "gesamt_article_count = df['full_article'].dropna().shape[0]\n",
        "\n",
        "Topic Modelling - Individual and Collective Analysis\n",
        "Next, we developed a topic model for each individual news outlet at the article level using the German-language model de_core_news_sm from spaCy. For each outlet, 20 distinct topics were generated. To ensure meaningful results, stopwords were removed prior to modeling, which was then conducted using lda_model. Visualizations were created by applying colormaps to the resulting graphs.\n",
        "In a subsequent step, we attempted to apply topic modeling at the paragraph level. For this, we installed sentence-transformers and scikit-learn, and used the BERT model 'all-MiniLM-L6-v2' to heuristically segment the texts in the CSV file into paragraphs. However, this process was ultimately unsuccessful, as no results were produced even after several hours of processing time.\n",
        "To complement the article-level topic modeling, we conducted an additional modeling step using a combined dataset containing all news outlets. This was done by merging individual CSV files stored in Google Drive. Using Google Colab, we first mounted the drive (*from google.colab import drive*, *drive.mount('/content/drive')*). Each file was read and then merged into a single DataFrame. Finally, the unified dataset was saved as a new CSV file: This consolidated file served as the basis for overall topic modeling across all sources.  To then analyze topic representation across different news outlets, we first computed topic distributions for each document using *lda_model.get_document_topics()*. After converted them into a DataFrame and adding the corresponding source information, we calculated the average topic distribution per outlet:  *avg_topic_distribution = topic_df.groupby(\"source\")[topic_cols].mean()*. To identify the top-performing outlets per topic, we extracted the top 2 sources for each topic and stored them in a summary table. We manually reordered the sources for clarity according to political orientation from left to right. Finally, we created a heatmap using Seaborn to show topic intensity across outlets.\n",
        "Our objective here was to understand which topics co-occur within the same news articles, in order to identify whether certain news outlets associate specific topics with the different parties involved in the Gaza conflict more consistently—or inconsistently—than their counterparts.\n",
        "\n",
        "Keyword Analysis - Genocide, Palestine and War Crimes\n",
        "\n",
        "In this step, we examined how frequently the entities Genozid, Kriegsverbrechen (war crime), and Palästina appeared in the reporting of each news outlet over time. For each article, we counted occurrences using regular expressions:  *df['genozid_count'] = df['full_article'].str.count(r'(?i)genozid')*, and similarly for the other terms. These counts were then resampled to calculate monthly totals:\n",
        " *df['genozid_count'].resample('M').sum()*. To normalize the data, we divided each monthly entity count by the number of articles published in that month, yielding relative mention rates per article: *genozid_relativ = genozid_monatlich / artikel_pro_monat*. Finally, we visualized the average mentions per article and per month in a line plot using Matplotlib, allowing us to compare how often each entity was referenced over time by the outlet.\n",
        "These terms were selected because their usage can serve as an early indicator of how news outlets frame the Gaza conflict—whether by implying accusations of unlawful conduct by Israel (genocide, war crime), or by suggesting a recognition of Palestinian legitimacy (Palästina).\n",
        "Contextual Analysis - Entity Context Window Analysis\n",
        "In the final step, we analyzed which nouns and proper nouns appear immediately before and after the terms Genozid and the conflict parties in the Gaza conflict, within articles from each news outlet. The choice of these entities is explained in Part 2 of this methodology. Using spaCy (*spacy.load('de_core_news_sm')*), we first extracted all nouns from the articles. We then searched for the target word (*genozid*) and collected the surrounding nouns within a context window of five words:  *kontext_fenster = 5*, *kontext = all_tokens[start:i] + all_tokens[i+1:end]*. Frequencies of these contextual nouns were counted using Python’s *Counter*, and the top 10 most common ones were visualized in a word cloud.\n",
        "We used the term genocide to analyze the surrounding context in which it is discussed, whether it appears alongside more critical or accusatory nouns, or in reference to historical events. Similarly, we examined the main entities involved in the Gaza conflict to assess whether they are reported in proximity to more violent, war-related, or emotionally charged language. This helps reveal how different parties are linguistically framed across news coverage.\n",
        "\n"
        "Limits\n",
        "\n",
        "Several limitations affected the implementation and scope of this analysis. First, paragraph-level topic modelling could not be completed due to extremely long processing times, which made it infeasible to run the code successfully. Since individual paragraphs often address specific subtopics, this limitation may have introduced inaccuracies in our article-level modelling approach. Second, the newspaper BILD contributed significantly fewer articles within the analyzed time frame. This data imbalance may have led to less reliable results for this outlet, particularly in the topic modelling and contextual analysis. Additionally, certain entities such as UNRWA and ICC were mentioned too infrequently to allow for meaningful interpretation of their linguistic context. Finally, due to unresolved technical errors, most parts of the analysis, except for the overall topic modelling, had to be conducted in separate Colab notebooks and using separate files. This fragmentation limited the possibility of a fully integrated codebase for the project.\n",
        "\n",
        "\n",
        "Attribution Analysis Detecting Blame and Criticism\n",
        "\n",
        "The objective of our attribution analysis is to identify how blame and criticism have been assigned - both explicitly and implicitly - to specific actors and entities in the reporting on the war in Gaza across the selected German media outlets.\n",
        "\n",
        "While the analysis does not seek to determine whether newspapers approve of or intentionally disseminate these criticisms and accusations, it does detect acts of quoting, amplifying or highlighting critical voices, which constitute an indirect form of attribution of responsibility. The aim is therefore to uncover and compare these discursive mechanisms of reproach across different outlets to better understand the differential roles German media outlets play in constructing and shaping narratives around accountability.\n",
        "\n",
        "Entity Identification\n",
        "\n",
        "We first identified key relevant entities that might be subjects of blame or criticism:\n",
        "\n",
        "Political Leaders: Netanyahu, Yoav Gallant\n",
        "Armed Groups and Military: Hamas, Israeli Defense Forces (IDF)\n",
        "States and Governments: Israel, Iran\n",
        "International Organizations and Tribunals: International Criminal Court (ICC), UN Relief and Works Agency (UNRWA)\n",
        "\n",
        "For each entity, we included multiple variations of their names or references as they might appear in German media (e.g., “Netanjahu”, “Netanyahu”, “Israels Premierminister”; “IDF”, “israelische Armee”, “israelische Verteidigungskräfte”, “Zahal”, “Israel Defense Forces”). We then organized these entities into standardized categories, enabling consistent analysis regardless of how an entity was referenced in the text.\n",
        "\n",
        "Linguistic Pattern Recognition\n",
        "\n",
        "We developed a sophisticated system to detect language patterns that indicate blame or criticism in German text. This involved several complementary approaches:\n",
        "Regular Expression Patterns\n",
        "\n",
        "We created 18 specific regular expression patterns designed to capture German grammatical constructions that signal blame attribution, including;\n",
        "Direct criticism constructions: kritisier[te]? (?:den|die|das)? ?({entities})\n",
        "Accusatory phrases: wirf[tf]? (?:den|die|das)? ?({entities}) vor\n",
        "Blame assignment: beschuldig[te]? (?:den|die|das)? ?({entities})\n",
        "Passive voice criticism: (?:den|die|das)? ?({entities}) wird vorgeworfen\n",
        "Responsibility attribution: (?:den|die|das)? ?({entities}) werden? verantwortlich gemacht\n",
        "Each pattern accounted for German grammatical variations, gender agreement, and tense forms to ensure comprehensive coverage.\n",
        "Keyword and Verb-Based Analysis\n",
        "\n",
        "We then compiled extensive lists of German verbs and keywords that signal blame or criticism:\n",
        "\n",
        "Blame verbs: kritisieren, vorwerfen, beschuldigen, angreifen, tadeln, verurteilen, anprangern, anklagen, bezichtigen, rügen, beanstanden, verdammen\n",
        "Responsibility keywords: schuld, verantwortlich, misstrauen, missbilligen\n",
        "Target verbs: angegriffen, beschuldigt, kritisiert, vorgeworfen, verfolgt, attackiert\n",
        "\n",
        "c) Dependency Parsing\n",
        "Using the German language model from spaCy (de_core_news_md), we performed dependency parsing to analyze the grammatical structure of sentences. This allowed us to:\n",
        "Identify when blame verbs had our target entities as their objects\n",
        "Detect more complex blame constructions that might not be captured by regular expressions\n",
        "Consider the broader context around entity mentions to determine if blame was being attributed\n",
        "3. Sentence Extraction and Analysis\n",
        "For each article, our system:\n",
        "Split the text into individual sentences\n",
        "Applied both pattern matching and dependency parsing to each sentence\n",
        "Identified sentences containing blame attribution to our target entities\n",
        "Categorized which entities were being blamed in each instance\n",
        "Stored both the statistical information and the actual sentences for qualitative review\n",
        "\n",
        "4. Multi-Source Comparative Analysis\n",
        "\n",
        "A key feature in our methodology is the ability to analyze and compare blame attribution patterns across different German news publications. With source-level aggregation, we tracked entity blame counts separately for each news source. Moreover, we normalized raw counts to account for differences in article volume across sources. Finally, we developed metrics to directly compare how different sources distribute blame as well as to calculate the average number of blame instances per article for each source.\n",
        "\n",
        "5. Visualization and Statistical Analysis\n",
        "\n",
        "Our methodology produced several sophisticated visualizations, namely comparative bar charts, showing the absolute frequency of blame attribution by entity across different sources, normalized percentage charts, displaying each source’s proportional distribution of blame, blame density charts, illustrating how intensively each source attributes blame, source-specific pie charts, breaking down the distribution of blamed entities within each individual source and timeline analysis, tracking how blame patterns evolved over the course of the conflict.\n",
        "\n",
        "Limits\n",
        "\n",
        "While our analysis aims to uncover how blame and criticism are assigned - explicitly and implicitly - across different German media outlets, it is important to recognize the methodological limits of this approach. While our focus on attribution, rather than intention, allows us to map how narratives of accountability appear across different German sources, our model does not systematically capture criticism expressed directly by the newspaper’s editorial voice. As a result, forms of more overt or unmediated criticism may go undetected, while the presence of many attributed statements may reflect a cautious or distanced reporting style.\n",
        "\n",
        "Furthermore, the model occasionally misattributes blame due to its inability to interpret the broader context or rhetorical purpose of a quotation. Thus, a core limitation of our approach is that while we capture the presence and frequency of attributed criticism, we do not always distinguish whether these attributions function to amplify, distance from or delegitimize the critical statement.\n",
        "\n"
      ],
      "metadata": {
        "id": "mPtu0pQJ1cFb"
      }
    }
  ]
}
